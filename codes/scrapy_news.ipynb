{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "1. Fazer scrapy de uma pagina de noticias\n",
    "2. Deixa somente os conteudo relevantes\n",
    "3. Armazenar os dados no BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "from functions import *\n",
    "from gcp_auth import credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurando o Webscraping\n",
    "\n",
    "1. Primeiro, o código define duas variáveis:\n",
    "    * `url`: Armazena a URL da página que será acessada, neste caso, a página da Globo.\n",
    "    * `headers`: É um dicionário que contém informações sobre o navegador que está fazendo a requisição. Aqui, define o `User-Agent` como \"Mozilla/5.0\", simulando um navegador comum.\n",
    "\n",
    "1. Obtendo o HTML da página:\n",
    "    * A linha `response = requests.get(url, headers=headers)` usa a biblioteca requests para fazer uma requisição GET à URL especificada no url.\n",
    "1. Extraindo dados com BeautifulSoup:\n",
    "\n",
    "    * A linha `page = BeautifulSoup(response.text, 'html.parser')` usa a biblioteca BeautifulSoup para transformar o texto HTML da resposta em um objeto que pode ser facilmente navegado e analisado.\n",
    "1. Selecionando a tag dos links:\n",
    "    * Por fim, o código usa o método `find` do objeto `soup` para encontrar a todas as tag (`a`) na página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.globo.com/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "page = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "urls = page.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Selecionando somente as tags que contem links de noticias \n",
    "* As variaveis `tag_class1` e `tag_class2` armazenam strings que representam nomes de classes em elementos HTML.\n",
    "*  Essas classes são usadas para identificar elementos que contêm links para artigos de notícias utilizadas pela Globo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias: 64\n"
     ]
    }
   ],
   "source": [
    "tag_class1 = \"post__title\"\n",
    "tag_class2 = \"post-multicontent__link--title__text\"\n",
    "\n",
    "news_url_list = []\n",
    "for url in urls:\n",
    "    if(url.h2 != None) and (url.h2.get(\"class\") != None):\n",
    "        if tag_class1 in url.h2.get(\"class\"):\n",
    "            news_url_list.append(url.get(\"href\"))\n",
    "        elif tag_class2 in url.h2.get(\"class\"):\n",
    "            news_url_list.append(url.get(\"href\"))\n",
    "\n",
    "print(f'Quantidade de notícias: {len(news_url_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deduplicacao dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvei o dataset que estava no BigQuery em uma planilha, em uma variavel chamada `df_scrapy_news_antigo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapy_news_antigo = pd.read_csv(\"data/bigquery_news_data.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transformo a variável `news_url_list` em um DataFrame chamado `df_urls`.\n",
    "1. Comparo o DataFrame `df_urls` com o DataFrame `df_scrapy_news_antigo` com base na coluna <strong>url</strong>, retornando assim somente as URLs novas, que não estão no BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = pd.DataFrame(news_url_list, columns=['url']) \n",
    "df_url_deduplicado = df_urls[~df_urls['url'].isin(df_scrapy_news_antigo['url'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformo o dataframe `df_url_deduplicado` em uma lista para conseguir usar a funcao `df_news_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias novas: 0\n"
     ]
    }
   ],
   "source": [
    "list_urls_novos = df_url_deduplicado['url'].tolist()\n",
    "print(f'Quantidade de notícias novas: {len(list_urls_novos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Raspagem do conteúdo das notícias\n",
    "\n",
    "Nessa parte estou utilizando uma função chamada `df_news_data`, onde os processos estão mais detalhados abaixo\n",
    "\n",
    "Processos:\n",
    "\n",
    "1. Loop: A função df_news_data utiliza um loop para iterar por cada URL na lista <strong>list_urls_novos.</strong>\n",
    "\n",
    "1. Requisição HTTP: Para cada URL, a função faz uma requisição HTTP para recuperar o conteúdo HTML da página de notícia.\n",
    "\n",
    "1. Análise de HTML: O HTML da página de notícia é então analisado para extrair os dados desejados:\n",
    "\n",
    "    * url: A URL da própria notícia.\n",
    "    * date: A data da publicação da notícia.\n",
    "    * author: O autor da notícia (se disponível).\n",
    "    * title: O título da notícia.\n",
    "    * subtitle: O subtítulo da notícia (se disponível).\n",
    "    * text: O corpo principal do texto da notícia.\n",
    "    \n",
    "1. Armazenamento de Dados: Os dados extraídos para cada notícia são armazenados em um dicionário.\n",
    "\n",
    "1. Agregação de Dados: O dicionário é transformado em um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio da raspagem de dados\n",
      "Coleta finalizada\n",
      "Quantidade de notícias novas coletas: 0\n"
     ]
    }
   ],
   "source": [
    "df_scrapy_news_novo = df_news_data(list_urls_novos)\n",
    "print(f'Quantidade de notícias novas coletas: {len(df_scrapy_news_novo)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Salvo os dados novos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concateno o dataset ja existente `df_scrapy_news_antigo` com o dataset com as novas URLs `df_scrapy_news_novo` e salvo novamente na mesma planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar os DataFrames df_scrapy_news_antigo e df_new__novo_deduplicado \n",
    "df_news_concat = pd.concat([df_scrapy_news_antigo, df_scrapy_news_novo]) \n",
    "df_news_concat.to_csv(\"data/bigquery_news_data.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adiciono os novos dados coletados no BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserindo 0 notícias novas no BigQuery\n"
     ]
    }
   ],
   "source": [
    "# Insere os dados novos no BigQuery\n",
    "print(f'Inserindo {len(df_scrapy_news_novo)} notícias novas no BigQuery')\n",
    "pandas_gbq.to_gbq(df_scrapy_news_novo, \n",
    "                  \"bigquery_news.tb_news_data\", \n",
    "                  project_id=\"project-scrapy-news\",\n",
    "                  if_exists=\"append\",\n",
    "                  credentials=credentials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
