{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "1. Fazer scrapy de uma pagina de noticias\n",
    "2. Deixa somente os conteudo relevantes\n",
    "3. Armazenar os dados no BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "from functions import *\n",
    "from gcp_auth import credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurando o Webscraping\n",
    "\n",
    "1. Primeiro, o código define duas variáveis:\n",
    "    * `url`: Armazena a URL da página que será acessada, neste caso, a página da Globo.\n",
    "    * `headers`: É um dicionário que contém informações sobre o navegador que está fazendo a requisição. Aqui, define o `User-Agent` como \"Mozilla/5.0\", simulando um navegador comum.\n",
    "\n",
    "1. Obtendo o HTML da página:\n",
    "    * A linha `response = requests.get(url, headers=headers)` usa a biblioteca requests para fazer uma requisição GET à URL especificada no url.\n",
    "1. Extraindo dados com BeautifulSoup:\n",
    "\n",
    "    * A linha `page = BeautifulSoup(response.text, 'html.parser')` usa a biblioteca BeautifulSoup para transformar o texto HTML da resposta em um objeto que pode ser facilmente navegado e analisado.\n",
    "1. Selecionando a tag dos links:\n",
    "    * Por fim, o código usa o método `find` do objeto `soup` para encontrar a todas as tag (`a`) na página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.globo.com/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "page = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "urls = page.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Selecionando somente as tags que contem links de noticias \n",
    "* As variaveis `tag_class1` e `tag_class2` armazenam strings que representam nomes de classes em elementos HTML.\n",
    "*  Essas classes são usadas para identificar elementos que contêm links para artigos de notícias utilizadas pela Globo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de notícias: 67\n"
     ]
    }
   ],
   "source": [
    "tag_class1 = \"post__title\"\n",
    "tag_class2 = \"post-multicontent__link--title__text\"\n",
    "\n",
    "news_url_list = []\n",
    "for url in urls:\n",
    "    if(url.h2 != None) and (url.h2.get(\"class\") != None):\n",
    "        if tag_class1 in url.h2.get(\"class\"):\n",
    "            news_url_list.append(url.get(\"href\"))\n",
    "        elif tag_class2 in url.h2.get(\"class\"):\n",
    "            news_url_list.append(url.get(\"href\"))\n",
    "\n",
    "print(f'Quantidade de notícias: {len(news_url_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raspagem do conteúdo das notícias\n",
    "\n",
    "Nessa parte estou utilizando uma função chamada `df_news_data`, onde os processos estão mais detalhados abaixo\n",
    "\n",
    "Processo:\n",
    "\n",
    "1. Loop: A função df_news_data utiliza um loop para iterar por cada URL na lista news_url_list.\n",
    "\n",
    "1. Requisição HTTP: Para cada URL, a função faz uma requisição HTTP para recuperar o conteúdo HTML da página de notícia.\n",
    "\n",
    "1. Análise de HTML: O HTML da página de notícia é então analisado para extrair os dados desejados:\n",
    "\n",
    "    * url: A URL da própria notícia.\n",
    "    * date: A data da publicação da notícia.\n",
    "    * author: O autor da notícia (se disponível).\n",
    "    * title: O título da notícia.\n",
    "    * subtitle: O subtítulo da notícia (se disponível).\n",
    "    * text: O corpo principal do texto da notícia.\n",
    "    \n",
    "1. Armazenamento de Dados: Os dados extraídos para cada notícia são armazenados em um dicionário.\n",
    "\n",
    "1. Agregação de Dados: Todos os dicionários de notícias individuais são concatenados em um único dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio da raspagem de dados\n",
      "Erro ao fazer scraping da URL https://extra.globo.com/entretenimento/noticia/2024/05/apos-segunda-cirurgia-exame-de-tony-ramos-aponta-importante-melhora-diz-boletim-medico.ghtml: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Coleta finalizada\n"
     ]
    }
   ],
   "source": [
    "df_news = df_news_data(news_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Salvando os dados coletados no BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "pandas_gbq.to_gbq(df_news, \n",
    "                  \"bigquery_news.news_data\", \n",
    "                  project_id=\"project-scrapy-news\",\n",
    "                  if_exists=\"replace\",\n",
    "                  credentials=credentials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
